# walmartsales-pipeline
ETL pipeline that cleans and transforms Walmart Sales data and stores in AWS S# bucket

ğŸ›’ Walmart Sales ETL Pipeline

Python â€¢ Pandas â€¢ Boto3 â€¢ Logging â€¢ AWS S3

This repository contains an ETL (Extract, Transform, Load) pipeline that processes Walmart sales data from a CSV file, cleans and transforms the dataset using Pandas, logs every step of the process using Pythonâ€™s logging module, and uploads the final cleaned file to an AWS S3 bucket using Boto3.

ğŸ“Œ Features

Extraction

Reads raw Walmart sales CSV files.

Logs start/end of extraction and file validation.

Transformation

Cleans missing or inconsistent values.

Converts date and numeric columns.

Creates new calculated fields like revenue.

Logs each transformation step for transparency.

Loading

Exports cleaned dataset to JSON or CSV.

Uploads processed file to AWS S3 using Boto3.

Logs upload status and success/failure messages.

ğŸ—‚ï¸ Project Structure
.
â”œâ”€â”€ data/
â”‚   â””â”€â”€ walmart_sales_raw.csv
â”œâ”€â”€ etl_pipeline.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

âš™ï¸ Technologies Used

Python 3.x

Pandas â€“ data cleaning and transformation

Boto3 â€“ AWS S3 integration

Logging â€“ detailed ETL run logs

AWS S3 â€“ cloud storage destination

ğŸš€ How to Run the Pipeline
1. Install Dependencies
pip install -r requirements.txt

2. Configure AWS Credentials

Use one of these methods:

aws configure

Environment variables

.aws/credentials file

IAM user must have PutObject permissions on your S3 bucket.

3. Run the ETL Script
python etl_pipeline.py


Logs will generate in the terminal or in your specified .log file.

ğŸ§¼ Example Transformation Steps

Remove duplicates

Convert â€œDateâ€ column to datetime

Ensure numeric columns (units sold, prices) are correctly typed

Compute revenue column

Standardize formatting of strings and categories

â˜ï¸ AWS S3 Upload

The pipeline uses boto3.client("s3") to upload:

s3.upload_file(
    "cleaned_walmart_sales.json",
    "your-s3-bucket",
    "walmart/cleaned_walmart_sales.json"
)

ğŸ“ Example etl_pipeline.py Outline
import pandas as pd
import boto3
import logging

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s"
)

def extract(file_path):
    logging.info("Starting data extraction...")
    df = pd.read_csv(file_path)
    logging.info(f"Extracted {len(df)} rows.")
    return df

def transform(df):
    logging.info("Starting data transformation...")

    df.drop_duplicates(inplace=True)
    df["Date"] = pd.to_datetime(df["Date"])
    df["Revenue"] = df["Units_Sold"] * df["Unit_Price"]

    logging.info("Transformation complete.")
    return df

def load(df, bucket, key):
    output_file = "cleaned_walmart_sales.json"
    df.to_json(output_file, orient="records")

    logging.info(f"Uploading {output_file} to S3...")
    s3 = boto3.client("s3")
    s3.upload_file(output_file, bucket, key)
    logging.info("Upload successful!")

if __name__ == "__main__":
    raw_df = extract("data/walmart_sales_raw.csv")
    clean_df = transform(raw_df)
    load(clean_df, "your-s3-bucket", "walmart/cleaned_walmart_sales.json")

ğŸ“ˆ Ideal Use Cases

Data engineering portfolio project

Learning ETL workflows with Python

Practicing S3 and cloud storage integration

Automating retail data cleanup tasks
